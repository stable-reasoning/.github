<!--
This file controls the landing page at https://github.com/stable-reasoning
Path: .github/profile/README.md
-->

<p align="center">
  <!-- Optional: drop a logo file in .github/profile/logo.png and uncomment -->
  <!-- <img src="./logo.png" width="96" alt="stable-reasoning logo" /> -->
</p>

<h1 align="center">Stable Reasoning</h1>
<p align="center"><i>Open, rigorous research into trustworthy reasoning systems.</i></p>

<p align="center">
  <!-- Replace placeholders as needed -->
  <a href="https://github.com/stable-reasoning"><img alt="Org" src="https://img.shields.io/badge/org-stable--reasoning-black"></a>
  <img alt="Focus" src="https://img.shields.io/badge/focus-AI%20reasoning-blue">
  <img alt="Open Science" src="https://img.shields.io/badge/open-science-brightgreen">
  <!-- Pick your license once decided -->
  <!-- <img alt="License" src="https://img.shields.io/badge/license-MIT-informational"> -->
</p>

---

## Mission

We study how to make machine reasoning **stable, transparent, and reliable**.  
Our goals:
- **Robustness:** mitigate brittleness, distribution shift, and adversarial quirks in chain-of-thought and tool use.
- **Evaluation:** build principled benchmarks and analyses for reasoning quality, not just accuracy.
- **Interpretability:** probe intermediate representations and step-by-step computations.
- **Open science:** release data, code, and write-ups for reproducibility.

## What we do

- **Empirical research:** experiments on reasoning traces, tool-augmented inference, and verification.
- **Method work:** training/decoding strategies (self-verification, deliberation, debate, search).
- **Evaluation assets:** datasets, metrics, and harnesses tailored to reasoning stability.
- **Community artifacts:** baselines, dashboards, and concise reports.

## Publications & Reports

- Preprints / whitepapers will be listed here with DOIs or arXiv IDs.
- Short technical notes live in repoâ€™s `/reports` folder.

